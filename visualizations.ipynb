{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip -q install torch-geometric\n",
    "!pip -q install nilearn"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from nilearn import plotting\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)                     # Python\n",
    "    np.random.seed(seed)                  # NumPy\n",
    "    torch.manual_seed(seed)               # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)          # PyTorch GPU\n",
    "    torch.cuda.manual_seed_all(seed)      # All GPUs (if using DataParallel or DDP)\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, SAGEConv, HeteroConv, Linear\n",
    "\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"You are using `torch.load`\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ],
   "id": "e66701e8bb660810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.graph_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_map = {\"M\": 0, \"F\":1}\n",
    "\n",
    "        for class_name, label in class_map.items():\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for graph_file in glob(os.path.join(class_path, \"*.pt\")):\n",
    "                self.graph_paths.append(graph_file)\n",
    "                self.labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.graph_paths[idx]\n",
    "        data = torch.load(path, weights_only=False)\n",
    "\n",
    "        data.y = torch.tensor([self.labels[idx]], dtype=torch.float)\n",
    "\n",
    "        desired_dim = 114\n",
    "        current_dim = data['roi'].x.shape[1]\n",
    "        if current_dim < desired_dim:\n",
    "            pad_size = desired_dim - current_dim\n",
    "            pad = torch.zeros((data['roi'].x.size(0), pad_size), dtype=data['roi'].x.dtype)\n",
    "            data['roi'].x = torch.cat([data['roi'].x, pad], dim=1)\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ],
   "id": "27d88c53847c84ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "graph_path = \"/content/drive/MyDrive/HCP Data/gender_graphs\"\n",
    "\n",
    "dataset = GraphDataset(graph_path)"
   ],
   "id": "30ab724d591fbee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class multihead(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.heads = 2\n",
    "\n",
    "        self.cluster_encoder = torch.nn.Sequential(\n",
    "            Linear(2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_channels)\n",
    "        )\n",
    "        self.roi_encoder = torch.nn.Sequential(\n",
    "            Linear(114, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(hidden_channels)\n",
    "        )\n",
    "\n",
    "        # Heterogeneous SAGE\n",
    "        self.sage = HeteroConv({\n",
    "            ('cluster', 'intersects', 'roi'): SAGEConv((-1, -1), hidden_channels),\n",
    "            ('roi', 'intersects_rev', 'cluster'): SAGEConv((-1, -1), hidden_channels),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Heterogeneous GAT\n",
    "        self.gat = HeteroConv({\n",
    "            ('cluster', 'intersects', 'roi'): GATConv((-1, -1), hidden_channels, heads=self.heads, concat=True, add_self_loops=False),\n",
    "            ('roi', 'intersects_rev', 'cluster'): GATConv((-1, -1), hidden_channels, heads=self.heads, concat=True, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.classifier = torch.nn.Linear(hidden_channels * 2 * self.heads, 1)\n",
    "\n",
    "    def forward(self, data, return_attention=False):\n",
    "        # Encode input features\n",
    "        x_dict = {\n",
    "            'cluster': self.cluster_encoder(data['cluster'].x),\n",
    "            'roi': self.roi_encoder(data['roi'].x),\n",
    "        }\n",
    "\n",
    "        # convolutions\n",
    "        x_dict = self.sage(x_dict, data.edge_index_dict)\n",
    "        x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "\n",
    "        if return_attention:\n",
    "            att_dict = {}\n",
    "            x_gat = {}\n",
    "            for edge_type, conv in self.gat.convs.items():\n",
    "                edge_index = data.edge_index_dict[edge_type]\n",
    "                out, (edge_index_used, attn_weights) = conv(\n",
    "                    (x_dict[edge_type[0]], x_dict[edge_type[2]]),\n",
    "                    edge_index,\n",
    "                    return_attention_weights=True\n",
    "                )\n",
    "                x_gat[edge_type[2]] = out  # aggregate to target node\n",
    "                att_dict[edge_type] = (edge_index_used, attn_weights)\n",
    "            x_dict = x_gat\n",
    "            x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "        else:\n",
    "            x_dict = self.gat(x_dict, data.edge_index_dict)\n",
    "            x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "            att_dict = None\n",
    "\n",
    "        # Global pooling\n",
    "        cluster_pool = global_mean_pool(x_dict['cluster'], data['cluster'].batch)\n",
    "        roi_pool = global_mean_pool(x_dict['roi'], data['roi'].batch)\n",
    "\n",
    "        # Classification\n",
    "        x = torch.cat([cluster_pool, roi_pool], dim=1)\n",
    "\n",
    "        if return_attention:\n",
    "            return self.classifier(x), att_dict\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = multihead(64)\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/HCP Data/Models/multihead/seed_4/best_model.pt\", weights_only=True, map_location=torch.device(device)))\n",
    "model.eval()"
   ],
   "id": "e6e7708d01b958c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "roi_attention_sum = defaultdict(lambda: defaultdict(float))\n",
    "roi_attention_count = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    data = data.to(device)\n",
    "    _, att_dict = model(data, return_attention=True)\n",
    "    edge_index, attn_weights = att_dict[('cluster', 'intersects', 'roi')]  # Shape: [E, heads]\n",
    "\n",
    "\n",
    "    for tgt_idx, att_vec in zip(edge_index[1].tolist(), attn_weights.tolist()):\n",
    "        for head_idx, score in enumerate(att_vec):\n",
    "            roi_attention_sum[tgt_idx][head_idx] += score\n",
    "            roi_attention_count[tgt_idx][head_idx] += 1\n",
    "\n",
    "avg_attention = {\n",
    "    roi: {\n",
    "        head: roi_attention_sum[roi][head] / roi_attention_count[roi][head]\n",
    "        for head in roi_attention_sum[roi]\n",
    "    }\n",
    "    for roi in roi_attention_sum\n",
    "}\n",
    "\n",
    "parcellation_nii = nib.load(\"aparc+aseg.nii.gz\")\n",
    "parcellation_data = parcellation_nii.get_fdata()\n",
    "\n",
    "roi_labels = np.unique(parcellation_data)\n",
    "roi_labels = roi_labels[roi_labels != 0]\n",
    "\n",
    "roi_label_to_index = {label: idx for idx, label in enumerate(roi_labels)}\n",
    "index_to_label = {v: k for k, v in roi_label_to_index.items()}\n",
    "\n",
    "\n",
    "headwise_label_attention = defaultdict(dict)  # head -> label -> score\n",
    "\n",
    "for roi_idx, head_scores in avg_attention.items():\n",
    "    if roi_idx not in index_to_label:\n",
    "        continue  # skip unknown ROI indices\n",
    "    label = index_to_label[roi_idx]\n",
    "    for head_idx, score in head_scores.items():\n",
    "        headwise_label_attention[head_idx][label] = score\n",
    "\n",
    "\n",
    "# Create NIfTI volume for each head\n",
    "output_paths = []\n",
    "for head_idx, label_score_map in headwise_label_attention.items():\n",
    "    attention_volume = np.zeros_like(parcellation_data)\n",
    "\n",
    "    for label, score in label_score_map.items():\n",
    "        attention_volume[parcellation_data == label] = score\n",
    "\n",
    "    out_img = nib.Nifti1Image(attention_volume, parcellation_nii.affine)\n",
    "    out_path = f\"gat_attention_head{head_idx}.nii.gz\"\n",
    "    nib.save(out_img, out_path)\n",
    "    output_paths.append(out_path)\n",
    "\n",
    "print(\"Saved attention maps to:\")\n",
    "for path in output_paths:\n",
    "    print(path)\n"
   ],
   "id": "df4d10fef520e0dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print top 5 regions per head\n",
    "N = 5\n",
    "for head_idx in headwise_label_attention:\n",
    "    print(f\"\\nTop {N} ROIs for Head {head_idx}:\")\n",
    "    top_items = sorted(headwise_label_attention[head_idx].items(), key=lambda x: x[1], reverse=True)[:N]\n",
    "    for label, score in top_items:\n",
    "        print(f\"  Label {label}: Score = {score:.4f}\")\n"
   ],
   "id": "df1e15a4348adb8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#heatmap visualizing top regions\n",
    "\n",
    "for path in output_paths:\n",
    "    display = plotting.plot_glass_brain(\n",
    "        path,\n",
    "        threshold=0.03,\n",
    "        display_mode='lyrz',\n",
    "        colorbar=True,\n",
    "        plot_abs=False\n",
    "    )\n",
    "    display.savefig(path.replace(\".nii.gz\", \"_glass.png\"))"
   ],
   "id": "34f2af9abebaf774"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
